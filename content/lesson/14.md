+++
title = "Disaggregated Memory"
[extra]
index = 14
thread = 60
+++

We talked about several forms of disaggregated memory:
- The memory blade approach proposed in the ISCA '09 paper (which can use cache-granularity or page-granularity transfers)
- Simple paging-based far memory (see [Infiniswap (NSDI '17)](https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/gu) and [FastSwap (EuroSys '20)](https://amyousterhout.com/papers/cfm_eurosys20.pdf), where the kernel page fault handler handles network transfers for pages
- [CXL memory pooling](https://computeexpresslink.org/blog/explaining-cxl-memory-pooling-and-sharing-1049/)

For the second approach, note that page size is not always the right answer. Sometimes, if applications prefer small object accesses, this
can lead to *I/O amplification*, where you're transfering more data (e.g., a page) over the network than you actually need. [AIFM (OSDI '20)](https://www.usenix.org/conference/osdi20/presentation/ruan) gets around this by using a lightweight user-space runtime and a special remote data structure library. Our work, [TrackFM (ASPLOS '24)](https://dl.acm.org/doi/10.1145/3617232.3624856) expands on this idea but makes it easier on developers by having the compiler transform the code automatically. [Mira (SOSP '23)](https://dl.acm.org/doi/10.1145/3600006.3613157) does something similar using profiling. 

We also spent some time talking about interconnect topologies, both for cluster-scale interconnects, and networks-on-chip (NoCs). A good reference
text for this is the [Dally and Towles book](https://www.amazon.com/Principles-Practices-Interconnection-Networks-Architecture/dp/0122007514). 

On readings:
Recommended background readings are marked with (^) above. Optional historical or fun readings are marked with (*). 
If you feel confortable with the topic already, you may skip these readings. 

You can find my slides [here](https://canvas.oregonstate.edu/courses/2028992/files/116618124?module_item_id=26534959)




